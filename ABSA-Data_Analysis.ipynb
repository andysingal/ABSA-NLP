{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab70d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://intellica-ai.medium.com/aspect-based-sentiment-analysis-everything-you-wanted-to-know-1be41572e238\n",
    "#https://medium.datadriveninvestor.com/sentiment-analysis-is-not-enough-ffafc8bbbfeb\n",
    "#https://www.kaggle.com/code/phiitm/aspect-based-sentiment-analysis/notebook\n",
    "#https://medium.com/analytics-vidhya/aspect-based-sentiment-analysis-a-practical-approach-8f51029bbc4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770bdf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (4.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (2.27.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Collecting en-core-web-sm==3.3.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (62.3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: stanfordnlp in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from stanfordnlp) (2.27.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from stanfordnlp) (1.11.0)\n",
      "Requirement already satisfied: tqdm in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from stanfordnlp) (4.64.0)\n",
      "Requirement already satisfied: protobuf in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from stanfordnlp) (3.19.4)\n",
      "Requirement already satisfied: numpy in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from stanfordnlp) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->stanfordnlp) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests->stanfordnlp) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests->stanfordnlp) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests->stanfordnlp) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from requests->stanfordnlp) (2.0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ankush.singal/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2022.3.15)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install tweepy\n",
    "!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en_core_web_lg\n",
    "!pip install stanfordnlp\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import warnings\n",
    "import time\n",
    "import csv\n",
    "import base64\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from textblob import TextBlob, Word\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words()\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# load a trained English pipeline \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import stanfordnlp\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "# initiate tqdm for pandas.apply() functions\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dff6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress all warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# expand notebook display options for dataframes\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bfe5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #twitter credentials\n",
    "# consumer_key = \"EhOZiQEfg2b8HfBeKjhX9ezB6\"\n",
    "# consumer_secret = \"jbJcZi5g5qGSYrIGzaatDuiYPg8QpYfURdjmfbfgZZk5RUYvgb\"\n",
    "# access_token = \"232661448-pdfmND03rvRkbPDWPfUGxuHY8ZHOENkNLSHZarPr\"\n",
    "# access_token_secret = \"CFAcNGLqMM24Nh8m3uQglb6CSpYU5vZtTp9j4Xvd7bB5S\"\n",
    "# hashtag='oneplus7pro'\n",
    "# tweet_count=50\n",
    "\n",
    "# # #Authenticate credentials\n",
    "# # auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "# # auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "# # api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "# # tweet_created = []\n",
    "# # tweet_text = []\n",
    "# # tweets = tweepy.Cursor(api.search_tweets,q='hare krsna',count=30, lang=\"en\")\n",
    "# # for tweet in tweets.items():\n",
    "# #     tweet_created.append(tweet.created_at)\n",
    "# #     tweet_text.append(tweet.text)  \n",
    "\n",
    "# # df = pd.DataFrame(columns=['created_on','tweets'])\n",
    "# # df['created_on'] = tweet_created\n",
    "# # df['tweets']= tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #method to fetch tweets from twitter\n",
    "# def get_tweets(hashtag, consumer_key, consumer_secret, access_token, access_token_secret, tweet_count):\n",
    "#     #authentication using credentials\n",
    "#     auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "#     auth.set_access_token(access_token, access_token_secret)\n",
    "#     api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "#     #fetching tweets holding specific hashtag\n",
    "#     tweet_text, tweet_created = list(), list()\n",
    "#     for tweet in tweepy.Cursor(api.search_tweets,q=hashtag, count=tweet_count, lang=\"en\").items():\n",
    "#         tweet_text.append(tweet.text.encode('utf-8'))\n",
    "#         tweet_created.append(tweet.created_at)\n",
    "       \n",
    "#     #creating dataframe    \n",
    "#     df = pd.DataFrame({'tweets': tweet_text, 'created_on': tweet_created})\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_content = get_tweets(hashtag, consumer_key, consumer_secret, access_token, access_token_secret, tweet_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b162305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('databricks2-Copy1.csv',usecols=['title'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c81d6",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Extracted tweet text may contain hashtags, tagging account, emojis, web-site/HTTP links, and HTML tags that need to be removed in pre-processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399fac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a0818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urlextract import URLExtract\n",
    "extractor = URLExtract()\n",
    "import contractions\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "# dictionary = {'\" \"':  ' ', \",\": ' ','-' : ' ',' \" ': ' ','()' : '',\n",
    "#                   '%' : '','#': '','\\[':'','\\]': '',\n",
    "#                   '\\\\(\\\\)':'','\\*':' ' ,'\\|':'',\n",
    "#                  '!':'','\\?':'','\\(\\)': '','/': '',\n",
    "#                   '\\)' : '', '\\(' : '', '\"': '', ':':'',\"'\": '','  +':' ',\n",
    "#                  }\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    #text = text.replace(dictionary,regex=True)\n",
    "    #to lowercase\n",
    "    text = str(text).lower()\n",
    "    #remove urls\n",
    "    urls = extractor.find_urls(text)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, '')\n",
    "    #remove new lines\n",
    "    text = text.replace('\\\\n',' ')\n",
    "    #replacement\n",
    "    text = text.replace(re.escape(\"\\]\\[\"), \"\")\n",
    "    text = text.replace(re.escape(\"]\"), \"\")\n",
    "    #remove none meaningful    \n",
    "    text = re.sub(r'\\\\x[0-9a-f]{2}', '',text)\n",
    "    #remove emails\n",
    "    text = re.sub(r'\\S*@\\S*\\s?',' ',text)\n",
    "    #remove mentions\n",
    "    text = re.sub(r'@\\S+', ' ', text)\n",
    "    #contractions\n",
    "    text = contractions.fix(text)\n",
    "    #remove hashtags\n",
    "    text = re.sub(r'@\\S+', ' ', text)\n",
    "    #remove emojis\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    #remove retweets\n",
    "    text = text.replace(r'rt', '')\n",
    "    #remove all punct\n",
    "    text = re.sub('[^A-za-z0-9]', ' ', text)\n",
    "    #remove extras whitespaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    #text = cleaning(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eaeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['title'] = preprocess(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b2ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba55784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning (column):\n",
    "    dictionary = {'\" \"':  ' ', \",\": ' ','-' : ' ',' \" ': ' ','()' : '',\n",
    "                  '%' : '','#': '','\\[':'','\\]': '',\n",
    "                  '\\\\(\\\\)':'','\\*':' ' ,'\\|':'',\n",
    "                 '!':'','\\?':'','\\(\\)': '','/': '',\n",
    "                  '\\)' : '', '\\(' : '', '\"': '', ':':'',\"'\": '','  +':' ',\n",
    "                 }\n",
    "    column = column.replace(dictionary, regex=True)\n",
    "    #column = column.str.lower()\n",
    "    \n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bec398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = cleaning(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=df['title'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580efeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a09297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sentiment polarity of each sentence.\n",
    "sentiment_scores = list()\n",
    "i = 0\n",
    "for sentence in result:\n",
    "    line = TextBlob(sentence)\n",
    "    sentiment_scores.append(line.sentiment.polarity)\n",
    "    if(i <= 10):\n",
    "        print(sentence + \": POLARITY=\" + str(line.sentiment.polarity))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot these sentiments!\n",
    "sns.distplot(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5aacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = TextBlob(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947eaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out noun phrases, will be useful for frequent feature extraction\n",
    "comments.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compactness pruning:\n",
    "cleaned = list()\n",
    "for phrase in comments.noun_phrases:\n",
    "    count = 0\n",
    "    for word in phrase.split():\n",
    "        # Count the number of small words and words without an English definition\n",
    "        if len(word) <= 2 or (not Word(word).definitions):\n",
    "            count += 1\n",
    "    # Only if the 'nonsensical' or short words DO NOT make up more than 40% (arbitrary) of the phrase add\n",
    "    # it to the cleaned list, effectively pruning the ones not added.\n",
    "    if count < len(phrase.split())*0.4:\n",
    "        cleaned.append(phrase)\n",
    "        \n",
    "print(\"After compactness pruning:\\nFeature Size:\")\n",
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b679c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase in cleaned:    \n",
    "    match = list()\n",
    "    temp = list()\n",
    "    word_match = list()\n",
    "    for word in phrase.split():\n",
    "        # Find common words among all phrases\n",
    "        word_match = [p for p in cleaned if re.search(word, p) and p not in word_match]\n",
    "        # If the size of matched phrases set is smaller than 30% of the cleaned phrases, \n",
    "        # then consider the phrase as non-redundant.\n",
    "        if len(word_match) <= len(cleaned)*0.3 :\n",
    "            temp.append(word)\n",
    "            match += word_match\n",
    "            \n",
    "    phrase = ' '.join(temp)\n",
    "    print(\"Match for \" + phrase + \": \" + str(match))\n",
    "\n",
    "    if len(match) >= len(cleaned)*0.1 :\n",
    "        # Redundant feature set, since it contains more than 10% of the number of phrases. \n",
    "        # Prune all matched features.\n",
    "        for feature in match:\n",
    "            if feature in cleaned:\n",
    "                cleaned.remove(feature)\n",
    "            \n",
    "        # Add largest length phrase as feature\n",
    "        cleaned.append(max(match, key=len))\n",
    "        \n",
    "\n",
    "print(\"After redundancy pruning:\\nFeature Size:\" + str(len(cleaned)))\n",
    "print(\"Cleaned features:\")\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496df638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "feature_count = dict()\n",
    "for phrase in cleaned:\n",
    "    count = 0\n",
    "    for word in phrase.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            count += comments.words.count(word)\n",
    "    \n",
    "    print(phrase + \": \" + str(count))\n",
    "    feature_count[phrase] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select frequent feature threshold as (max_count)/100 \n",
    "# This is an arbitrary decision as of now.\n",
    "counts = list(feature_count.values())\n",
    "features = list(feature_count.keys())\n",
    "threshold = len(comments.noun_phrases)/100\n",
    "\n",
    "print(\"Threshold:\" + str(threshold))\n",
    "\n",
    "frequent_features = list()\n",
    "\n",
    "for feature, count in feature_count.items():\n",
    "    if count >= threshold:\n",
    "        frequent_features.append(feature)\n",
    "        \n",
    "print('Frequent Features:')\n",
    "frequent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [feat for feat in features][:20]\n",
    "cnt = [count for count in counts][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07854e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot these feature occurences and draw the threshold line\n",
    "sns.set()\n",
    "sns.set_context(\"poster\")\n",
    "f, ax = plt.subplots(figsize=(10, 13))\n",
    "sns.swarmplot(y=feats, x=cnt, color=\"c\", ax=ax)\n",
    "plt.plot([threshold, threshold], [0, len(features)], linewidth=4, color=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/mlp9/Aspect-based-opinion-mining-NLP-with-Python-/blob/main/aspect_based_opinion_mining.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96465f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_sentiment(sentence):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    \n",
    "    nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "    score = nltk_sentiment.polarity_scores(sentence)\n",
    "    return score\n",
    "#b=dataset.values.T.tolist()\n",
    "#print(b)\n",
    "nltk_results = [nltk_sentiment(row) for row in frequent_features]\n",
    "#print(nltk_results)\n",
    "results_df = pd.DataFrame(nltk_results)\n",
    "#print(results_df)\n",
    "text_df = pd.DataFrame(frequent_features)\n",
    "#print(text_df)\n",
    "nltk_df = text_df.join(results_df)\n",
    "#nltk_df1=nltk_df[[0,'neu']]\n",
    "#print(nltk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ed5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf=nltk_df[0]\n",
    "newdf=pd.DataFrame({'features':nltk_df[0],'pos':nltk_df['pos'],'neg':nltk_df['neg']})\n",
    "newdf.pos=newdf.pos+0.2\n",
    "newdf.neg=newdf.neg-0.2\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bcc815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "newdf\n",
    "\n",
    "#noun=['positive attitude','good job knowledge','team player','customer acquisition','good visibility','back office process','successful launch','soft skills','suitable candidates','core issues']\n",
    "#pos_l=[0.71,0.42,0.3,0.2,0.2,0.6,0.2,0.5,0.43,0]\n",
    "#neg_l=[-0.1,0,-0.300,0,-0.1,0,-0.1,-0.2,0,-0.3]\n",
    "\n",
    "pos=newdf[0:5]['pos']\n",
    "neg=newdf[0:5]['neg']\n",
    "\n",
    "# data to plot\n",
    "n_groups = 5\n",
    "positive =newdf['pos'].head(5) \n",
    "negative =newdf['neg'].head(5)\n",
    " \n",
    "# create plot\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.3\n",
    "opacity = 1\n",
    " \n",
    "rects1 = plt.bar(index, positive, bar_width,\n",
    "alpha=opacity,\n",
    "color='b',\n",
    "label='positive sentiments')\n",
    " \n",
    "rects2 = plt.bar(index + bar_width, negative, bar_width,\n",
    "alpha=opacity,\n",
    "color='r',\n",
    "label='negative sentiments')\n",
    " \n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('sentiment value')\n",
    "plt.title('Top features and its sentiment')\n",
    "plt.xticks(index + bar_width, newdf['features'].head(5))\n",
    "plt.xticks(rotation = 90)\n",
    "plt.legend()\n",
    "fig.set_size_inches(20, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc893ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "absa_list = dict()\n",
    "# For each frequent feature\n",
    "for f in frequent_features:\n",
    "    # For each comment\n",
    "    absa_list[f] = list()\n",
    "    for comment in result:\n",
    "        blob = TextBlob(comment)\n",
    "        # For each sentence of the comment\n",
    "        for sentence in blob.sentences:\n",
    "            # Search for frequent feature 'f'\n",
    "            q = '|'.join(f.split())\n",
    "            if re.search(r'\\w*(' + str(q) + ')\\w*', str(sentence)):\n",
    "                absa_list[f].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "absa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list()\n",
    "absa_scores = dict()\n",
    "for k, v in absa_list.items():\n",
    "    absa_scores[k] = list()\n",
    "    for sent in v:\n",
    "        score = sent.sentiment.polarity\n",
    "        scores.append(score)\n",
    "        absa_scores[k].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec97cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70862977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have all the scores, let's plot them!\n",
    "# For comparison, we replot the previous global sentiment polarity plot\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(20, 10))\n",
    "plot1 = sns.distplot(scores, ax=ax1)\n",
    "\n",
    "ax1.set_title('Aspect wise scores')\n",
    "ax1.set_xlabel('Sentiment Polarity')\n",
    "ax1.set_ylabel('# of comments')\n",
    "\n",
    "ax2.set_title('Comment wise scores')\n",
    "ax2.set_xlabel('Sentiment Polarity')\n",
    "ax2.set_ylabel('# of comments')\n",
    "\n",
    "plot2 = sns.distplot(sentiment_scores, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data values for stripplot and boxplot\n",
    "vals = dict()\n",
    "vals[\"aspects\"] = list()\n",
    "vals[\"scores\"] = list()\n",
    "for k, v in absa_scores.items():\n",
    "    for score in v:\n",
    "        vals[\"aspects\"].append(k)\n",
    "        vals[\"scores\"].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2 = {\n",
    "    \"aspects\" : vals[\"aspects\"][:1500],\n",
    "    \"scores\" : vals[\"scores\"][:1500]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(30, 10))\n",
    "\n",
    "color = sns.color_palette(\"Blues\", 6)\n",
    "plt.xticks(rotation=90)\n",
    "sns.set_context(\"paper\", font_scale=3) \n",
    "sns.boxplot(x=\"aspects\", y=\"scores\", data=vals2, palette=color, ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2846cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = sns.color_palette(\"Reds\", 6)\n",
    "fig, ax1 = plt.subplots(figsize=(30, 10))\n",
    "plt.xticks(rotation=90)\n",
    "sns.set_context(\"paper\", font_scale=2) \n",
    "sns.stripplot(x=\"aspects\", y=\"scores\",data=vals2, palette=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5edf9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
